{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from optformer.decoding_regression import models\n",
    "from optformer.decoding_regression import vocabs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define a constant for negative infinity\n",
    "NEG_INF = float('-inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the vocabulary.\n",
    "vocab = vocabs(size=7, token_length=7)\n",
    "\n",
    "# Convert each target to a token sequence.\n",
    "Y_token_ids = np.array([vocab.to_int(y) for y in Y])\n",
    "print(\"Example target:\", Y[0])\n",
    "print(\"Example token IDs:\", Y_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "# Score function needed for evaluation of the model in the competition\n",
    "# Define the concordance index function\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n",
    "    >>> y_pred = pd.DataFrame(y_pred)\n",
    "    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n",
    "    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n",
    "    >>> y_true = pd.DataFrame(y_true)\n",
    "    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n",
    "    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n",
    "    0.75\n",
    "    \"\"\"\n",
    "\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    for col in submission.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical features\n",
    "def encode_categorical_features(df, cats, encoding_maps=None):\n",
    "    \"\"\"\n",
    "    Encode categorical features in a df using label encoding.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing categorical features.\n",
    "    cats (list): List of column names in the DataFrame to be encoded.\n",
    "    encoding_maps (dict, optional): Dictionary to store encoding mappings for test dataframe. \n",
    "                                    If None, a new dictionary will be created.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with encoded categorical features.\n",
    "    dict: Dictionary containing encoding mappings for each categorical feature.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Store encoding mappings for consistency in test data\n",
    "    if encoding_maps is None:\n",
    "        encoding_maps = {}  \n",
    "    \n",
    "    print(\"Label Encoding:\")\n",
    "    for c in cats:\n",
    "        if c not in encoding_maps:\n",
    "            # Sort ensures stable encoding\n",
    "            df[c], mapping = df[c].factorize(sort=True) \n",
    "            encoding_maps[c] = {val: idx for idx, val in enumerate(mapping)}\n",
    "        else:\n",
    "            df[c] = df[c].map(encoding_maps[c])\n",
    "        \n",
    "        # Ensure all mapped values are valid integers\n",
    "        # Assign -1 for unseen categories in test data\n",
    "        df[c] = df[c].fillna(-1).astype(int)  \n",
    "        \n",
    "        print(f'{c}: n_categories={len(encoding_maps[c].values())}, mapped values={list(encoding_maps[c].values())}')\n",
    "  \n",
    "    return df, encoding_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this notebook https://www.kaggle.com/code/ambrosm/esp-eda-which-makes-sense/notebook\n",
    "def transform_quantile(time, event):\n",
    "    \"\"\"Transform the target by stretching the range of eventful efs_times and compressing the range of event_free efs_times\n",
    "\n",
    "    From https://www.kaggle.com/code/ambrosm/esp-eda-which-makes-sense\"\"\"\n",
    "    transformed = np.full(len(time), np.nan)\n",
    "    transformed_dead = quantile_transform(- time[event == 1].values.reshape(-1, 1)).ravel()\n",
    "    transformed[event == 1] = transformed_dead\n",
    "    transformed[event == 0] = transformed_dead.min() - 0.3\n",
    "    return transformed\n",
    "    \n",
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "    \n",
    "\n",
    "def transform_survival_target(df, time_col='efs_time', event_col='efs'):\n",
    "    df[\"y3\"] = df.efs_time.values\n",
    "    mx = df.loc[df.efs==1,\"efs_time\"].max()\n",
    "    mn = df.loc[df.efs==0,\"efs_time\"].min()\n",
    "    df.loc[df.efs==0,\"y3\"] = df.loc[df.efs==0,\"y3\"] + mx - mn\n",
    "    df.y3 = df.y3.rank()\n",
    "    df.loc[df.efs==0,\"y3\"] += 2*len(df)\n",
    "    df.y3 = df.y3 / df.y3.max()\n",
    "    df.y3 = np.log( df.y3 )\n",
    "    df.y3 -= df.y3.mean()\n",
    "    df.y3 *= -1.0\n",
    "    return df.y3\n",
    "\n",
    "df_train[\"y1\"] = transform_survival_probability(df_train, time_col='efs_time', event_col='efs')\n",
    "df_train[\"y2\"] = transform_quantile(time=df_train['efs_time'], event=df_train['efs'])\n",
    "df_train[\"y3\"] = transform_survival_target(df_train, time_col='efs_time', event_col='efs')\n",
    "\n",
    "# Plot for y1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_train.loc[df_train.efs==1, \"y1\"], bins=100, alpha=0.5, label=\"efs=1, Yes Event\")\n",
    "plt.hist(df_train.loc[df_train.efs==0, \"y1\"], bins=100, alpha=0.5, label=\"efs=0, Maybe Event\")\n",
    "plt.xlabel(\"Transformed Target y1\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Transformed Target y1 using survival probability.\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot for y2\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_train.loc[df_train.efs==1, \"y2\"], bins=100, alpha=0.5, label=\"efs=1, Yes Event\")\n",
    "plt.hist(df_train.loc[df_train.efs==0, \"y2\"], bins=100, alpha=0.5, label=\"efs=0, Maybe Event\")\n",
    "plt.xlabel(\"Transformed Target y2\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Transformed Target y2 using quantile transformation.\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot for y3\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_train.loc[df_train.efs==1, \"y3\"], bins=100, alpha=0.5, label=\"efs=1, Yes Event\")\n",
    "plt.hist(df_train.loc[df_train.efs==0, \"y3\"], bins=100, alpha=0.5, label=\"efs=0, Maybe Event\")\n",
    "plt.xlim((-5, 5))\n",
    "plt.xlabel(\"Transformed Target y3\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Transformed Target y3 using both efs and efs_time.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = df_train.select_dtypes(include=\"object\").columns.to_list()\n",
    "numerical_features = [col for col in df_train.select_dtypes(exclude=\"object\").columns if col not in [\"ID\",\"efs\",\"efs_time\",\"y1\",\"y2\",\"y3\"]]\n",
    "\n",
    "# Handle missing values\n",
    "for col in categorical_features:\n",
    "    df_train[col] = df_train[col].fillna(\"NAN\")\n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = df_test[col].fillna(\"NAN\")\n",
    "\n",
    "\n",
    "# Encode training data and save mapping\n",
    "train, encoding_maps = encode_categorical_features(df_train, cats=categorical_features)\n",
    "\n",
    "# Encode test data using saved mapping\n",
    "test, _ = encode_categorical_features(df_test, cats=categorical_features, encoding_maps=encoding_maps)\n",
    "\n",
    "\n",
    "# Use training median to impute nan values\n",
    "train = train.fillna(train[numerical_features].median())\n",
    "test = test.fillna(train[numerical_features].median())\n",
    "\n",
    "\n",
    "# Normalize numerical features\n",
    "for col in numerical_features:\n",
    "    m = train[col].mean()\n",
    "    s = train[col].std()\n",
    "    train[col] = (train[col] - m) / (s + 1e-8)  \n",
    "    \n",
    "    m = test[col].mean()\n",
    "    s = test[col].std()\n",
    "    test[col] = (test[col] - m) / (s + 1e-8)  \n",
    "\n",
    "\n",
    "# Define embedding sizes\n",
    "emb_c = {n: len(col.unique()) for n, col in train[categorical_features].items()}\n",
    "embedding_sizes = {col: (len(train[col].unique()), np.int64(np.sqrt(len(train[col].unique()) + 1))) for col in categorical_features}\n",
    "\n",
    "train_features = [c for c in train.columns if not c in [\"ID\",\"efs\",\"efs_time\",\"y1\",\"y2\",\"y3\"]]\n",
    "test_features = [c for c in test.columns if not c in [\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "\n",
    "# Convert categorical & numerical data to tensors\n",
    "X_cat = train[categorical_features].to_numpy()\n",
    "X_cont = train[numerical_features].to_numpy()\n",
    "y = df_train[\"y3\"].to_numpy()\n",
    "\n",
    "# Convert to tensors\n",
    "X_cat = torch.LongTensor(X_cat)\n",
    "X_cont = torch.tensor(X_cont, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Convert target values into token sequences\n",
    "Y_token_ids = np.array([vocab.to_int(y_val) for y_val in y])\n",
    "Y_token_ids = torch.tensor(Y_token_ids, dtype=torch.long)\n",
    "\n",
    "# Initialize arrays for OOF and test predictions\n",
    "oof_predictions = np.zeros(len(X_cat))\n",
    "predictions = []\n",
    "\n",
    "# Debugging: Print shapes\n",
    "print(f\"X_cat shape: {X_cat.shape}, X_cont shape: {X_cont.shape}, Y shape: {Y_token_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data & Feature Info\n",
    "n_cont = len(numerical_features)\n",
    "n_cat = len(categorical_features)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "NUM_EPOCHS = 5\n",
    "NUM_FOLDS = 5\n",
    "NUM_REPEATS = 1\n",
    "\n",
    "# Loss function (weighted sparse categorical cross entropy)\n",
    "loss_weights = torch.tensor([0.3, 0.3, 0.09, 0.01, 0.01, 0.3, 0.5], device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for metrics\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for repeat in range(NUM_REPEATS):\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_cat)):\n",
    "        print(f\"\\nFold {fold+1}/{NUM_FOLDS}\")\n",
    "\n",
    "        # Train-validation split for current fold\n",
    "        X_cat_train, X_cat_val = X_cat[train_idx], X_cat[val_idx]\n",
    "        X_cont_train, X_cont_val = X_cont[train_idx], X_cont[val_idx]\n",
    "        Y_train, Y_val = Y_token_ids[train_idx], Y_token_ids[val_idx]\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_ds = TensorDataset(X_cat_train, X_cont_train, Y_train[:, :-1], Y_train)\n",
    "        val_ds = TensorDataset(X_cat_val, X_cont_val, Y_val[:, :-1], Y_val)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=len(val_ds))\n",
    "\n",
    "        # Model initialization\n",
    "        model = AttentionDecoder(\n",
    "            encoder=DummyEncoder(),\n",
    "            vocab=vocab,\n",
    "            embedding_sizes=embedding_sizes,\n",
    "            units=128,\n",
    "            num_layers=1,\n",
    "            num_heads=1,\n",
    "            dropout=0.1,\n",
    "            encoder_dim=n_cont\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for x_cat_batch, x_cont_batch, dec_input, y_batch in train_loader:\n",
    "                x_cat_batch, x_cont_batch, dec_input, y_batch = (\n",
    "                    x_cat_batch.to(device),\n",
    "                    x_cont_batch.to(device),\n",
    "                    dec_input.to(device),\n",
    "                    y_batch.to(device),\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model((x_cont_batch, x_cat_batch, dec_input))\n",
    "                loss_tensor = weighted_sparse_categorical_crossentropy(y_batch, outputs, weights=loss_weights)\n",
    "                loss = loss_tensor.mean()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x_cat_batch, x_cont_batch, dec_input, y_batch in val_loader:\n",
    "                    x_cat_batch, x_cont_batch, dec_input, y_batch = (\n",
    "                        x_cat_batch.to(device),\n",
    "                        x_cont_batch.to(device),\n",
    "                        dec_input.to(device),\n",
    "                        y_batch.to(device),\n",
    "                    )\n",
    "                    outputs = model((x_cont_batch, x_cat_batch, dec_input))\n",
    "                    val_loss += weighted_sparse_categorical_crossentropy(y_batch, outputs, weights=loss_weights).mean().item()\n",
    "\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Store metrics\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        # Store OOF predictions\n",
    "        oof_predictions[val_idx] += model.decode(X_cont_val.to(device)).cpu().numpy().squeeze()\n",
    "\n",
    "        # Test predictions\n",
    "        test_cat = torch.LongTensor(test[categorical_features].to_numpy()).to(device)\n",
    "        test_cont = torch.tensor(test[numerical_features].to_numpy(), dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            test_preds = model.decode(test_cont).cpu().numpy()\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            predictions.append(test_preds)\n",
    "        else:\n",
    "            predictions += test_preds\n",
    "\n",
    "# Final aggregation\n",
    "oof_predictions[val_idx] = oof_predictions[val_idx] / (NUM_REPEATS * NUM_EPOCHS)\n",
    "predictions = np.array(predictions) / NUM_REPEATS\n",
    "\n",
    "# Final cross-validation results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Train Loss: {np.mean(train_losses):.4f} ± {np.std(train_losses):.4f}\")\n",
    "print(f\"Val Loss: {np.mean(val_losses):.4f} ± {np.std(val_losses):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_optformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
