# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Register necessary SeqIO Tasks/Mixtures.
#
# Configuration for models trained on the BBOB dataset with all HPO algorithms.
# Random objective range augmentation.
# Minimum configuration in meta data.
#
# inputs length = 256
# targets lengths = 2048
# max_trials = 200

from __gin__ import dynamic_registration

from optformer.data import converters
from t5x import utils

include 't5x/examples/t5/t5_1_1/base.gin'
include 'optformer/t5x/configs/models/t5_1_1_overrides.gin'

MIXTURE_OR_TASK_NAME = 'cached_bbob_train_repeat_4'
train_eval/utils.DatasetConfig:
  mixture_or_task_name = 'cached_bbob_eval'
USE_CACHED_TASKS = True

TRAIN_STEPS = 1_000_000
LEARNING_RATE = 0.3
DROPOUT_RATE = 0.0

TARGET_LENGTHS = 2048
TASK_FEATURE_LENGTHS = {
    'inputs': 256,
    'targets': %TARGET_LENGTHS,
    'target_inputs': %TARGET_LENGTHS,
    'targets_types': %TARGET_LENGTHS,
    'targets_masks': %TARGET_LENGTHS,
}

# Config for study converter.
STUDY_CONVERTER = @converters.OptFormerConverter()
converters.OptFormerConverter:
  rand_objective_scale_range = (0.3, 1.0)
  max_trials = 200
  num_initial_tokens = %NUM_INITIAL_TOKENS
  minimum_config = True  # Always minimum_config
  minimum_config_per_study = False
